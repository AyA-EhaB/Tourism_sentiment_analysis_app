{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMrG1Y7C7gLXGmYsOx+Z4l0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mosapmohamd/DEPI-Graduation-Project/blob/main/Text_Preprocessing_%26_Annotation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import required dependences"
      ],
      "metadata": {
        "id": "5L3sA2nhSqa1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mANNB8tQEEeY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import emoji\n",
        "import contractions\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6r2Te5SM64c",
        "outputId": "0ab08903-983f-4fda-ed88-cb4912962817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/590.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/590.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m583.7/590.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voJ6EToaPEV0",
        "outputId": "7e2fbacf-5d91-4340-c2f8-3addc17fb2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "id": "DnQg3J09MqO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Preprocessing**"
      ],
      "metadata": {
        "id": "eIREtSFlSagK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slang_dict = {\n",
        "    \"$\" : \" dollar \",\n",
        "    \"€\" : \" euro \",\n",
        "    \"4ao\" : \"for adults only\",\n",
        "    \"a.m\" : \"before midday\",\n",
        "    \"a3\" : \"anytime anywhere anyplace\",\n",
        "    \"aamof\" : \"as a matter of fact\",\n",
        "    \"acct\" : \"account\",\n",
        "    \"adih\" : \"another day in hell\",\n",
        "    \"afaic\" : \"as far as i am concerned\",\n",
        "    \"afaict\" : \"as far as i can tell\",\n",
        "    \"afaik\" : \"as far as i know\",\n",
        "    \"afair\" : \"as far as i remember\",\n",
        "    \"afk\" : \"away from keyboard\",\n",
        "    \"app\" : \"application\",\n",
        "    \"approx\" : \"approximately\",\n",
        "    \"apps\" : \"applications\",\n",
        "    \"asap\" : \"as soon as possible\",\n",
        "    \"asl\" : \"age, sex, location\",\n",
        "    \"atk\" : \"at the keyboard\",\n",
        "    \"ave.\" : \"avenue\",\n",
        "    \"aymm\" : \"are you my mother\",\n",
        "    \"ayor\" : \"at your own risk\",\n",
        "    \"b&b\" : \"bed and breakfast\",\n",
        "    \"b+b\" : \"bed and breakfast\",\n",
        "    \"b.c\" : \"before christ\",\n",
        "    \"b2b\" : \"business to business\",\n",
        "    \"b2c\" : \"business to customer\",\n",
        "    \"b4\" : \"before\",\n",
        "    \"b4n\" : \"bye for now\",\n",
        "    \"b@u\" : \"back at you\",\n",
        "    \"bae\" : \"before anyone else\",\n",
        "    \"bak\" : \"back at keyboard\",\n",
        "    \"bbbg\" : \"bye bye be good\",\n",
        "    \"bbc\" : \"british broadcasting corporation\",\n",
        "    \"bbias\" : \"be back in a second\",\n",
        "    \"bbl\" : \"be back later\",\n",
        "    \"bbs\" : \"be back soon\",\n",
        "    \"be4\" : \"before\",\n",
        "    \"bfn\" : \"bye for now\",\n",
        "    \"blvd\" : \"boulevard\",\n",
        "    \"bout\" : \"about\",\n",
        "    \"brb\" : \"be right back\",\n",
        "    \"bros\" : \"brothers\",\n",
        "    \"brt\" : \"be right there\",\n",
        "    \"bsaaw\" : \"big smile and a wink\",\n",
        "    \"btw\" : \"by the way\",\n",
        "    \"bwl\" : \"bursting with laughter\",\n",
        "    \"c/o\" : \"care of\",\n",
        "    \"cet\" : \"central european time\",\n",
        "    \"cf\" : \"compare\",\n",
        "    \"cia\" : \"central intelligence agency\",\n",
        "    \"csl\" : \"can not stop laughing\",\n",
        "    \"cu\" : \"see you\",\n",
        "    \"cul8r\" : \"see you later\",\n",
        "    \"cv\" : \"curriculum vitae\",\n",
        "    \"cwot\" : \"complete waste of time\",\n",
        "    \"cya\" : \"see you\",\n",
        "    \"cyt\" : \"see you tomorrow\",\n",
        "    \"dae\" : \"does anyone else\",\n",
        "    \"dbmib\" : \"do not bother me i am busy\",\n",
        "    \"diy\" : \"do it yourself\",\n",
        "    \"dm\" : \"direct message\",\n",
        "    \"dwh\" : \"during work hours\",\n",
        "    \"e123\" : \"easy as one two three\",\n",
        "    \"eet\" : \"eastern european time\",\n",
        "    \"eg\" : \"example\",\n",
        "    \"embm\" : \"early morning business meeting\",\n",
        "    \"encl\" : \"enclosed\",\n",
        "    \"encl.\" : \"enclosed\",\n",
        "    \"etc\" : \"and so on\",\n",
        "    \"faq\" : \"frequently asked questions\",\n",
        "    \"fawc\" : \"for anyone who cares\",\n",
        "    \"fb\" : \"facebook\",\n",
        "    \"fc\" : \"fingers crossed\",\n",
        "    \"fig\" : \"figure\",\n",
        "    \"fimh\" : \"forever in my heart\",\n",
        "    \"ft.\" : \"feet\",\n",
        "    \"ft\" : \"featuring\",\n",
        "    \"ftl\" : \"for the loss\",\n",
        "    \"ftw\" : \"for the win\",\n",
        "    \"fwiw\" : \"for what it is worth\",\n",
        "    \"fyi\" : \"for your information\",\n",
        "    \"g9\" : \"genius\",\n",
        "    \"gahoy\" : \"get a hold of yourself\",\n",
        "    \"gal\" : \"get a life\",\n",
        "    \"gcse\" : \"general certificate of secondary education\",\n",
        "    \"gfn\" : \"gone for now\",\n",
        "    \"gg\" : \"good game\",\n",
        "    \"gl\" : \"good luck\",\n",
        "    \"glhf\" : \"good luck have fun\",\n",
        "    \"gmt\" : \"greenwich mean time\",\n",
        "    \"gmta\" : \"great minds think alike\",\n",
        "    \"gn\" : \"good night\",\n",
        "    \"g.o.a.t\" : \"greatest of all time\",\n",
        "    \"goat\" : \"greatest of all time\",\n",
        "    \"goi\" : \"get over it\",\n",
        "    \"gps\" : \"global positioning system\",\n",
        "    \"gr8\" : \"great\",\n",
        "    \"gratz\" : \"congratulations\",\n",
        "    \"gyal\" : \"girl\",\n",
        "    \"h&c\" : \"hot and cold\",\n",
        "    \"hp\" : \"horsepower\",\n",
        "    \"hr\" : \"hour\",\n",
        "    \"hrh\" : \"his royal highness\",\n",
        "    \"ht\" : \"height\",\n",
        "    \"ibrb\" : \"i will be right back\",\n",
        "    \"im\" : \"i am\",\n",
        "    \"ic\" : \"i see\",\n",
        "    \"icq\" : \"i seek you\",\n",
        "    \"icymi\" : \"in case you missed it\",\n",
        "    \"idc\" : \"i do not care\",\n",
        "    \"idgadf\" : \"i do not give a damn fuck\",\n",
        "    \"idgaf\" : \"i do not give a fuck\",\n",
        "    \"idk\" : \"i do not know\",\n",
        "    \"ie\" : \"that is\",\n",
        "    \"i.e\" : \"that is\",\n",
        "    \"ifyp\" : \"i feel your pain\",\n",
        "    \"IG\" : \"instagram\",\n",
        "    \"iirc\" : \"if i remember correctly\",\n",
        "    \"ilu\" : \"i love you\",\n",
        "    \"ily\" : \"i love you\",\n",
        "    \"imho\" : \"in my humble opinion\",\n",
        "    \"imo\" : \"in my opinion\",\n",
        "    \"imu\" : \"i miss you\",\n",
        "    \"iow\" : \"in other words\",\n",
        "    \"irl\" : \"in real life\",\n",
        "    \"j4f\" : \"just for fun\",\n",
        "    \"jic\" : \"just in case\",\n",
        "    \"jk\" : \"just kidding\",\n",
        "    \"jsyk\" : \"just so you know\",\n",
        "    \"l8r\" : \"later\",\n",
        "    \"lb\" : \"pound\",\n",
        "    \"lbs\" : \"pounds\",\n",
        "    \"ldr\" : \"long distance relationship\",\n",
        "    \"lmao\" : \"laugh my ass off\",\n",
        "    \"lmfao\" : \"laugh my fucking ass off\",\n",
        "    \"lol\" : \"laughing out loud\",\n",
        "    \"ltd\" : \"limited\",\n",
        "    \"ltns\" : \"long time no see\",\n",
        "    \"m8\" : \"mate\",\n",
        "    \"mf\" : \"motherfucker\",\n",
        "    \"mfs\" : \"motherfuckers\",\n",
        "    \"mfw\" : \"my face when\",\n",
        "    \"mofo\" : \"motherfucker\",\n",
        "    \"mph\" : \"miles per hour\",\n",
        "    \"mr\" : \"mister\",\n",
        "    \"mrw\" : \"my reaction when\",\n",
        "    \"ms\" : \"miss\",\n",
        "    \"mte\" : \"my thoughts exactly\",\n",
        "    \"nagi\" : \"not a good idea\",\n",
        "    \"nbc\" : \"national broadcasting company\",\n",
        "    \"nbd\" : \"not big deal\",\n",
        "    \"nfs\" : \"not for sale\",\n",
        "    \"ngl\" : \"not going to lie\",\n",
        "    \"nhs\" : \"national health service\",\n",
        "    \"nrn\" : \"no reply necessary\",\n",
        "    \"nsfl\" : \"not safe for life\",\n",
        "    \"nsfw\" : \"not safe for work\",\n",
        "    \"nth\" : \"nice to have\",\n",
        "    \"nvr\" : \"never\",\n",
        "    \"nyc\" : \"new york city\",\n",
        "    \"oc\" : \"original content\",\n",
        "    \"og\" : \"original\",\n",
        "    \"ohp\" : \"overhead projector\",\n",
        "    \"oic\" : \"oh i see\",\n",
        "    \"omdb\" : \"over my dead body\",\n",
        "    \"omg\" : \"oh my god\",\n",
        "    \"omw\" : \"on my way\",\n",
        "    \"p.a\" : \"per annum\",\n",
        "    \"p.m\" : \"after midday\",\n",
        "    \"pm\" : \"prime minister\",\n",
        "    \"poc\" : \"people of color\",\n",
        "    \"pov\" : \"point of view\",\n",
        "    \"pp\" : \"pages\",\n",
        "    \"ppl\" : \"people\",\n",
        "    \"prw\" : \"parents are watching\",\n",
        "    \"ps\" : \"postscript\",\n",
        "    \"pt\" : \"point\",\n",
        "    \"ptb\" : \"please text back\",\n",
        "    \"pto\" : \"please turn over\",\n",
        "    \"qpsa\" : \"what happens\", #\"que pasa\",\n",
        "    \"ratchet\" : \"rude\",\n",
        "    \"rbtl\" : \"read between the lines\",\n",
        "    \"rlrt\" : \"real life retweet\",\n",
        "    \"rofl\" : \"rolling on the floor laughing\",\n",
        "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
        "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
        "    \"rt\" : \"retweet\",\n",
        "    \"ruok\" : \"are you ok\",\n",
        "    \"sfw\" : \"safe for work\",\n",
        "    \"sk8\" : \"skate\",\n",
        "    \"smh\" : \"shake my head\",\n",
        "    \"sq\" : \"square\",\n",
        "    \"srsly\" : \"seriously\",\n",
        "    \"ssdd\" : \"same stuff different day\",\n",
        "    \"tbh\" : \"to be honest\",\n",
        "    \"tbs\" : \"tablespooful\",\n",
        "    \"tbsp\" : \"tablespooful\",\n",
        "    \"tfw\" : \"that feeling when\",\n",
        "    \"thks\" : \"thank you\",\n",
        "    \"tho\" : \"though\",\n",
        "    \"thx\" : \"thank you\",\n",
        "    \"tia\" : \"thanks in advance\",\n",
        "    \"til\" : \"today i learned\",\n",
        "    \"tl;dr\" : \"too long i did not read\",\n",
        "    \"tldr\" : \"too long i did not read\",\n",
        "    \"tmb\" : \"tweet me back\",\n",
        "    \"tntl\" : \"trying not to laugh\",\n",
        "    \"ttyl\" : \"talk to you later\",\n",
        "    \"u\" : \"you\",\n",
        "    \"u2\" : \"you too\",\n",
        "    \"u4e\" : \"yours for ever\",\n",
        "    \"utc\" : \"coordinated universal time\",\n",
        "    \"w/\" : \"with\",\n",
        "    \"w/o\" : \"without\",\n",
        "    \"w8\" : \"wait\",\n",
        "    \"wassup\" : \"what is up\",\n",
        "    \"wb\" : \"welcome back\",\n",
        "    \"wtf\" : \"what the fuck\",\n",
        "    \"wtg\" : \"way to go\",\n",
        "    \"wtpa\" : \"where the party at\",\n",
        "    \"wuf\" : \"where are you from\",\n",
        "    \"wuzup\" : \"what is up\",\n",
        "    \"wywh\" : \"wish you were here\",\n",
        "    \"yd\" : \"yard\",\n",
        "    \"ygtr\" : \"you got that right\",\n",
        "    \"ynk\" : \"you never know\",\n",
        "    \"zzz\" : \"sleeping bored and tired\"\n",
        "}"
      ],
      "metadata": {
        "id": "NnIA4_5wNhqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower()\n",
        "    text = emoji.demojize(text)\n",
        "    text = text.replace(\":\", \"\").replace(\"_\", \" \")\n",
        "    text = contractions.fix(text)\n",
        "\n",
        "    words = text.split()\n",
        "    processed_words = []\n",
        "    for word in words:\n",
        "        word_cleaned = word.strip(string.punctuation)\n",
        "        if word_cleaned.lower() in slang_dict:\n",
        "            processed_words.append(slang_dict[word_cleaned.lower()])\n",
        "        else:\n",
        "            processed_words.append(word)\n",
        "        text = \" \".join(processed_words)\n",
        "\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "        text = re.sub(r'[^\\w\\s\\.\\!\\?]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "fIdPx5PdOkF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_additional_features(df):\n",
        "    df['review_length'] = df['review'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)\n",
        "    df['word_count'] = df['review'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)\n",
        "    df['year'] = pd.to_datetime(df['date'], errors='coerce').dt.year\n",
        "    df['month'] = pd.to_datetime(df['date'], errors='coerce').dt.month\n",
        "    df['exclamation_count'] = df['review'].apply(lambda x: str(x).count('!') if pd.notna(x) else 0)\n",
        "    df['question_count'] = df['review'].apply(lambda x: str(x).count('?') if pd.notna(x) else 0)\n",
        "    return df"
      ],
      "metadata": {
        "id": "DVyV6zFoPMCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Egypt_Tourism_Reviews.csv')"
      ],
      "metadata": {
        "id": "I3h3SnUqPQh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " df['processed_review'] = df['review'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "dj0Z6LPjQ6Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = extract_additional_features(df)"
      ],
      "metadata": {
        "id": "pjQIHptAQ7Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('preprocessed_tourism_reviews.csv', index=False)\n",
        "print(f\"Preprocessing complete. Processed {len(df)} reviews.\")"
      ],
      "metadata": {
        "id": "IOQtC5stQ92l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data annotation**"
      ],
      "metadata": {
        "id": "r7otg__ASXz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import torch\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "NXPWUngBTnkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('preprocessed_tourism_reviewsv2.csv')\n",
        "df = df[df['word_count'] < 384]\n",
        "pos = df[df['sentiment'] == 'Positive'].sample(500)\n",
        "neg = df[df['sentiment'] == 'Negative'].sample(500)\n",
        "neu = df[df['sentiment'] == 'Neutral'].sample(500)\n",
        "df = pd.concat([pos, neg , neu])\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "35Bqf1ZGUUGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1: Binary Transformer Model (postive , negative)\n",
        "def label_with_binary_transformer(df, text_column, model_name=\"siebert/sentiment-roberta-large-english\"):\n",
        "    sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model_name, device=0 if device == 'cuda' else -1)\n",
        "    batch_size = 32\n",
        "    results = []\n",
        "    for i in tqdm(range(0, len(df), batch_size), desc=\"Binary Transformer\"):\n",
        "        batch = df[text_column].iloc[i:i+batch_size].tolist()\n",
        "        outputs = sentiment_analyzer(batch, truncation=True)\n",
        "        results.extend(outputs)\n",
        "    labels = [result['label'] for result in results]\n",
        "    scores = [result['score'] for result in results]\n",
        "    sentiment_map = {'POSITIVE': 'positive', 'NEGATIVE': 'negative'}\n",
        "    mapped_labels = [sentiment_map.get(label, label.lower()) for label in labels]\n",
        "    neutral_threshold = 0.9  # Adjustable threshold\n",
        "    final_labels = ['neutral' if score < neutral_threshold else label for label, score in zip(mapped_labels, scores)]\n",
        "    return final_labels, scores"
      ],
      "metadata": {
        "id": "GT78EWcPUYJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2: VADER Sentiment Analysis (positive , negative , neutral)\n",
        "def label_with_vader(df, text_column):\n",
        "    try:\n",
        "        nltk.data.find('sentiment/vader_lexicon.zip')\n",
        "    except LookupError:\n",
        "        nltk.download('vader_lexicon')\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    scores = []\n",
        "    for text in tqdm(df[text_column], desc=\"VADER\"):\n",
        "        if isinstance(text, str):\n",
        "            sentiment_dict = sid.polarity_scores(text)\n",
        "            scores.append(sentiment_dict)\n",
        "        else:\n",
        "            scores.append({'compound': 0, 'neg': 0, 'neu': 0, 'pos': 0})\n",
        "    compound_scores = [score['compound'] for score in scores]\n",
        "    labels = ['positive' if score >= 0.1 else 'negative' if score <= -0.1 else 'neutral' for score in compound_scores]\n",
        "    return labels, compound_scores"
      ],
      "metadata": {
        "id": "y-u1bry6Uaj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_with_three_class_model(df, text_column, model_name=\"cardiffnlp/twitter-roberta-base-sentiment\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
        "    print(f\"Three-Class Model is on device: {next(model.parameters()).device}\")\n",
        "    labels = []\n",
        "    scores = []\n",
        "    batch_size = 16\n",
        "    for i in tqdm(range(0, len(df), batch_size), desc=\"Three-Class Transformer\"):\n",
        "        batch_texts = df[text_column].iloc[i:i+batch_size].tolist()\n",
        "        batch_texts = [str(text) if isinstance(text, str) else \"\" for text in batch_texts]\n",
        "        encoded_batch = tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "        encoded_batch = {k: v.to(device) for k, v in encoded_batch.items()}  # Move to GPU\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_batch)\n",
        "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_classes = predictions.argmax(dim=1).tolist()\n",
        "        max_scores = predictions.max(dim=1).values.tolist()\n",
        "        batch_labels = ['negative' if label == 0 else 'neutral' if label == 1 else 'positive' for label in predicted_classes]\n",
        "        labels.extend(batch_labels)\n",
        "        scores.extend(max_scores)\n",
        "    return labels, scores"
      ],
      "metadata": {
        "id": "tCu0hx2eUd_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply models\n",
        "text_column = 'review'\n",
        "labels_binary, scores_binary = label_with_binary_transformer(df, text_column)\n",
        "labels_vader, scores_vader = label_with_vader(df, text_column)\n",
        "labels_three_class, scores_three_class = label_with_three_class_model(df, text_column)"
      ],
      "metadata": {
        "id": "3KVRVtGUUmd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment_binary'] = labels_binary\n",
        "df['sentiment_score_binary'] = scores_binary\n",
        "df['sentiment_vader'] = labels_vader\n",
        "df['sentiment_score_vader'] = scores_vader\n",
        "df['sentiment_three_class'] = labels_three_class\n",
        "df['sentiment_score_three_class'] = scores_three_class"
      ],
      "metadata": {
        "id": "VZWcyAnUUqJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensemble Method: Majority Voting\n",
        "def ensemble_sentiment(row):\n",
        "    sentiments = [row['sentiment_binary'], row['sentiment_vader'], row['sentiment_three_class']]\n",
        "    sentiment_counts = Counter(sentiments)\n",
        "    most_common = sentiment_counts.most_common(1)[0][0]\n",
        "    return most_common\n",
        "\n",
        "df['sentiment_ensemble'] = df.apply(ensemble_sentiment, axis=1)"
      ],
      "metadata": {
        "id": "tjqjr0OEUq3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('labeled_tourism_reviews.csv', index=False)"
      ],
      "metadata": {
        "id": "c7975XcHU0-4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}